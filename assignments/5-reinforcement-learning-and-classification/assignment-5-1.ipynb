{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"b8d4aefefc4f42108847f2113922d3b0","deepnote_cell_type":"markdown","id":"R3D82waLqItO"},"source":["# DAT565 Introduction to Data Science and AI \n","\n","- Student 1 - Luca Modica - (5 hours)\n","- Student 2 - Hugo Alves Henriques E Silva - (5 hours)\n","- Student 3 - YenPo Lin - (5 hours)\n","\n","## 2023-2024, LP1\n","## Assignment 5: Reinforcement Learning and Classification\n","\n","The exercise takes place in a notebook environment where you can chose to use Jupyter or Google Colabs. We recommend you use Google Colabs as it will facilitate remote group-work and makes the assignment less technical. \n","\n","The exercise takes place in this notebook environment where you can chose to use Jupyter or Google Colabs. We recommend you use Google Colabs as it will facilitate remote group-work and makes the assignment less technical. \n","\n","*Tips:* \n","* You can execute certain Linux shell commands by prefixing the command with a `!`. \n","* You can insert Markdown cells and code cells. The first you can use for documenting and explaining your results, the second you can use to write code snippets that execute the tasks required.  \n","\n","This assignment is about **sequential decision making** under uncertainty (reinforcement learning). In a sequential decision process, the process jumps between different states (the *environment*), and in each state the decision maker, or *agent*, chooses among a set of actions. Given the state and the chosen action, the process jumps to a new state. At each jump the decision maker receives a reward, and the objective is to find a sequence of decisions (or an optimal *policy*) that maximizes the accumulated rewards.\n","\n","We will use **Markov decision processes** (MDPs) to model the environment, and below is a primer on the relevant background theory. \n"]},{"cell_type":"markdown","metadata":{"cell_id":"7d8a72f1c8a24ff5ac4ef1ca16ffacde","deepnote_cell_type":"markdown","id":"8jEcC9NKqItQ"},"source":["\n","* To make things concrete, we will first focus on decision making under **no** uncertainity (questions 1 and 2), i.e, given we have a world model, we can calculate the exact and optimal actions to take in it. We will first introduce **Markov Decision Process (MDP)** as the world model. Then we give one algorithm (out of many) to solve it.\n","\n","* (optional) Next we will work through one type of reinforcement learning algorithm called Q-learning (question 3). Q-learning is an algorithm for making decisions under uncertainity, where uncertainity is over the possible world model (here MDP). It will find the optimal policy for the **unknown** MDP, assuming we do infinite exploration.\n","\n","* Finally, in question 4 you will be asked to explain differences between reinforcement learning and supervised learning and in question 5 write about decision trees and random forests."]},{"cell_type":"markdown","metadata":{"cell_id":"06d289eddcdf42e5800fc82aaa534039","deepnote_cell_type":"markdown","id":"uGtknnUVqItP"},"source":["## Primer\n","### Decision Making\n","The problem of **decision making under uncertainty** (commonly known as **reinforcement learning**) can be broken down into\n","two parts. First, how do we learn about the world? This involves both the\n","problem of modeling our initial uncertainty about the world, and that of drawing conclusions from evidence and our initial belief. Secondly, given what we\n","currently know about the world, how should we decide what to do, taking into\n","account future events and observations that may change our conclusions?\n","Typically, this will involve creating long-term plans covering possible future\n","eventualities. That is, when planning under uncertainty, we also need to take\n","into account what possible future knowledge could be generated when implementing our plans. Intuitively, executing plans which involve trying out new\n","things should give more information, but it is hard to tell whether this information will be beneficial. The choice between doing something which is already\n","known to produce good results and experiment with something new is known\n","as the **exploration-exploitation dilemma**.\n","\n","### The exploration-exploitation trade-off\n","\n","Consider the problem of selecting a restaurant to go to during a vacation. Lets say the\n","best restaurant you have found so far was **Les Epinards**. The food there is\n","usually to your taste and satisfactory. However, a well-known recommendations\n","website suggests that **King’s Arm** is really good! It is tempting to try it out. But\n","there is a risk involved. It may turn out to be much worse than **Les Epinards**,\n","in which case you will regret going there. On the other hand, it could also be\n","much better. What should you do?\n","It all depends on how much information you have about either restaurant,\n","and how many more days you’ll stay in town. If this is your last day, then it’s\n","probably a better idea to go to **Les Epinards**, unless you are expecting **King’s\n","Arm** to be significantly better. However, if you are going to stay there longer,\n","trying out **King’s Arm** is a good bet. If you are lucky, you will be getting much\n","better food for the remaining time, while otherwise you will have missed only\n","one good meal out of many, making the potential risk quite small."]},{"cell_type":"markdown","metadata":{"cell_id":"98415797abcf4759b2111994c19e8cd6","deepnote_cell_type":"markdown","id":"h9WIePUCqItR"},"source":["### Markov Decision Processes\n","Markov Decision Processes (MDPs) provide a mathematical framework for modeling sequential decision making under uncertainty. An *agent* moves between *states* in a *state space* choosing *actions* that affects the transition probabilities between states, and the subsequent *rewards* recieved after a jump. This is then repeated a finite or infinite number of epochs. The objective, or the *solution* of the MDP, is to optimize the accumulated rewards of the process.\n","\n","Thus, an MDP consists of five parts: \n","\n","* Decision epochs: $t={1,2,...,T}$, where $T\\leq \\infty$\n","* State space: $S=\\{s_1,s_2,...,s_N\\}$ of the underlying environment\n","* Action space $A=\\{a_1,a_2,...,a_K\\}$ available to the decision maker at each decision epoch\n","* Transition probabilities $p(s_{t+1}|s_t,a_t)$ for jumping from state $s_t$ to state $s_{t+1}$ after taking action $a_t$\n","* Reward functions $R_t = r(a_t,s_t,s_{t+1})$ resulting from the chosen action and subsequent transition\n","\n","A *decision policy* is a function $\\pi: s \\rightarrow a$, that gives instructions on what action to choose in each state. A policy can either be *deterministic*, meaning that the action is given for each state, or *randomized* meaning that there is a probability distribution over the set of possible actions for each state. Given a specific policy $\\pi$ we can then compute the the *expected total reward* when starting in a given state $s_1 \\in S$, which is also known as the *value* for that state, \n","\n","$$V^\\pi (s_1) = E\\left[ \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) {\\Large |} s_1\\right] = \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) p(s_{t+1} | a_t,s_t)$$ \n","\n","where $a_t = \\pi(s_t)$. To ensure convergence and to control how much credit to give to future rewards, it is common to introduce a *discount factor* $\\gamma \\in [0,1]$. For instance, if we think all future rewards should count equally, we would use $\\gamma = 1$, while if we value near-future rewards higher than more distant rewards, we would use $\\gamma < 1$. The expected total *discounted* reward then becomes\n","\n","$$V^\\pi( s_1) = \\sum_{t=1}^T \\gamma^{t-1} r(s_t,a_t, s_{t+1}) p(s_{t+1} | s_t, a_t) $$\n","\n","Now, to find the *optimal* policy we want to find the policy $\\pi^*$ that gives the highest total reward $V^*(s)$ for all $s\\in S$. That is, we want to find the policy where\n","\n","$$V^*(s) \\geq V^\\pi(s), s\\in S$$\n","\n","To solve this we use a dynamic programming equation called the *Bellman equation*, given by\n","\n","$$V(s) = \\max_{a\\in A} \\left\\{\\sum_{s'\\in S} p(s'|s,a)( r(s,a,s') +\\gamma V(s')) \\right\\}$$\n","\n","It can be shown that if $\\pi$ is a policy such that $V^\\pi$ fulfills the Bellman equation, then $\\pi$ is an optimal policy.\n","\n","A real world example would be an inventory control system. The states could be the amount of items we have in stock, and the actions would be the amount of items to order at the end of each month. The discrete time would be each month and the reward would be the profit. \n"]},{"cell_type":"markdown","metadata":{"cell_id":"9b0f24ea9486426382657e10bd6f125b","deepnote_cell_type":"markdown","id":"KiO_zpY7qItS"},"source":["## Question 1"]},{"cell_type":"markdown","metadata":{"cell_id":"9fab82807c8d45e8a1ef616851723eb1","deepnote_cell_type":"markdown","id":"XUyGq4olqItS"},"source":["The first question covers a deterministic MPD, where the action is directly given by the state, described as follows:\n","\n","* The agent starts in state **S** (see table below)\n","* The actions possible are **N** (north), **S** (south), **E** (east), and **W** west. \n","* The transition probabilities in each box are deterministic (for example P(s'|s,N)=1 if s' north of s). Note, however, that you cannot move outside the grid, thus all actions are not available in every box.\n","* When reaching **F**, the game ends (absorbing state).\n","* The numbers in the boxes represent the rewards you receive when moving into that box. \n","* Assume no discount in this model: $\\gamma = 1$\n","    \n","    \n","| -1 | 1  | F  |\n","| 0  | -1 | 1  |\n","| -1 | 0  | -1 |\n","| S  | -1 | 1  |\n","\n","Let $(x,y)$ denote the position in the grid, such that $S=(0,0)$ and $F=(2,3)$.\n","\n","**1a)** What is the optimal path of the MDP above? Is it unique? Submit the path as a single string of directions. For instance, NESW will make a circle.\n","\n","**1b)** What is the optimal policy (i.e., the optimal action in each state)? It is helpful if you draw the arrows/letters in the grid.\n","\n","**1c)** What is expected total reward for the policy in 1a)?\n"]},{"cell_type":"markdown","metadata":{"cell_id":"3a24500aba6c4ea7a4e71524bb57d7ef","deepnote_cell_type":"markdown"},"source":["---"]},{"cell_type":"code","execution_count":1,"metadata":{"cell_id":"59e6e44d17a847818ed70c4f216d042d","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":2733,"execution_start":1696083312117,"source_hash":"9b675205"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":2,"metadata":{"cell_id":"2076ac2592bb4cee895a2f844cbee0ae","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":26,"execution_start":1696083314860,"source_hash":"6d99f644"},"outputs":[],"source":["num_rows = 3\n","num_cols = 4\n","\n","grid = {\n","    (0, 3): -1, (1, 3): 1, (2, 3): 0,\n","    (0, 2): 0, (1, 2): -1, (2, 2): 1,\n","    (0, 1): -1, (1, 1): 0, (2, 1): -1,\n","    (0, 0): 0, (1, 0): -1, (2, 0): 1,\n","}\n","\n","moves = {'N': (0, 1), 'S': (0, -1), 'E': (1, 0), 'W': (-1, 0)}\n","\n","start = (0,0)\n","end = (2, 3)"]},{"cell_type":"markdown","metadata":{"cell_id":"959c90a2bc3543cbbbe68d205bdeed13","deepnote_cell_type":"markdown"},"source":["### Question 1a "]},{"cell_type":"code","execution_count":3,"metadata":{"cell_id":"edca0f42156d4289bc6152436ebc93ec","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":27,"execution_start":1696083314869,"source_hash":"65d167e4"},"outputs":[],"source":["def move_pos(pos, move):\n","    # create a copy of the position and move it\n","    x, y = pos\n","\n","    new = (x + moves[move][0], y + moves[move][1])\n","\n","    if (new[0] < 0 or new[0] > 2 or new[1] < 0 or new[1] > 3):\n","        # dont move, out of boundaries\n","        return pos\n","    return new"]},{"cell_type":"code","execution_count":4,"metadata":{"cell_id":"35527e50908e4862900fc2cd80d9b2c7","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":40,"execution_start":1696083314890,"source_hash":"96961d43"},"outputs":[],"source":["def findPath(grid, position, end, prev_moves, paths, rewards, reward, visited = None):\n","    \"\"\"\n","    find all paths going from a position to the end,\n","    with all the related accumulated rewards\n","    \"\"\"\n","\n","    if visited is None:\n","        visited = set()\n","    visited.add(position)\n","\n","    move_names = [key for key in moves] \n","    for move in move_names:\n","        new_pos = move_pos(position, move)\n","        if new_pos in visited:\n","            # dont go to visited positions\n","            continue\n","        if new_pos == end:\n","            # found a path\n","            paths.append(prev_moves + move)\n","            rewards.append(reward)\n","        else:\n","            findPath(grid, new_pos, end, prev_moves + move, paths, rewards, reward + grid[new_pos], visited.copy())\n","\n","    return paths, rewards"]},{"cell_type":"code","execution_count":5,"metadata":{"cell_id":"c2f63b1b994c4fe18619c5ce06cf8799","deepnote_cell_type":"code","deepnote_table_loading":false,"deepnote_table_state":{"filters":[],"pageIndex":0,"pageSize":100,"sortBy":[]},"deepnote_to_be_reexecuted":false,"execution_millis":336,"execution_start":1696083314948,"source_hash":"d0d8b923"},"outputs":[{"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":2,"columns":[{"dtype":"object","name":"path","stats":{"categories":[{"count":1,"name":"EENNN"},{"count":1,"name":"EENNWNE"}],"nan_count":0,"unique_count":2}},{"dtype":"int64","name":"total_reward","stats":{"histogram":[{"bin_end":-0.4,"bin_start":-0.5,"count":0},{"bin_end":-0.3,"bin_start":-0.4,"count":0},{"bin_end":-0.19999999999999996,"bin_start":-0.3,"count":0},{"bin_end":-0.09999999999999998,"bin_start":-0.19999999999999996,"count":0},{"bin_end":0,"bin_start":-0.09999999999999998,"count":0},{"bin_end":0.10000000000000009,"bin_start":0,"count":2},{"bin_end":0.20000000000000007,"bin_start":0.10000000000000009,"count":0},{"bin_end":0.30000000000000004,"bin_start":0.20000000000000007,"count":0},{"bin_end":0.4,"bin_start":0.30000000000000004,"count":0},{"bin_end":0.5,"bin_start":0.4,"count":0}],"max":"0","min":"0","nan_count":0,"unique_count":1}},{"dtype":"int64","name":"_deepnote_index_column"}],"row_count":2,"rows":[{"_deepnote_index_column":27,"path":"EENNN","total_reward":0},{"_deepnote_index_column":28,"path":"EENNWNE","total_reward":0}]},"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>path</th>\n","      <th>total_reward</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>27</th>\n","      <td>EENNN</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>EENNWNE</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       path  total_reward\n","27    EENNN             0\n","28  EENNWNE             0"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["paths, rewards = findPath(grid, start, end, '', [], [], 0)\n","\n","data = {'path': paths, 'total_reward': rewards}\n","df = pd.DataFrame(data).sort_values(by='total_reward', ascending=False)\n","\n","max_reward = df['total_reward'].max()\n","\n","# show the optimal path\n","df[df['total_reward'] == max_reward]"]},{"cell_type":"markdown","metadata":{"cell_id":"0a3662511a404be49e954d47b5e12664","deepnote_cell_type":"markdown"},"source":["There 2 optimal paths, hence not a unique optimal one:\n","\n","- E -> E -> N -> N -> N\n","- E -> E -> N -> N -> W -> N -> E"]},{"cell_type":"markdown","metadata":{"cell_id":"b24520d979be4961bf68a451dc046a05","deepnote_cell_type":"markdown"},"source":["### Question 1b"]},{"cell_type":"code","execution_count":6,"metadata":{"cell_id":"754c2efcab1d4ced8d30b129762acc2d","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":139,"execution_start":1696083315201,"source_hash":"1d54405e"},"outputs":[],"source":["def print_policy(policy):\n","    for y in range(num_cols):\n","        for x in range(num_rows):\n","            move = policy[x, num_cols - 1 -y]\n","            if move == '':\n","                print('  | ', end='')\n","            else:\n","                print(f'{move} | ', end='')\n","        print('')"]},{"cell_type":"code","execution_count":7,"metadata":{"cell_id":"901c81014d02404e95003a57bc903738","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":39,"execution_start":1696083315202,"source_hash":"2462d3c9"},"outputs":[],"source":["def calculate_state_value(pos, value_func):\n","    \"\"\"\n","    compute the the optimal state value, returning\n","    the acual state value and the related action to achieve it \n","    \"\"\"\n","    x, y = pos\n","\n","    if pos == end:\n","        return '', 0 # end state -> no action\n","    elif x < 0 or x >= num_rows or y < 0 or y >= num_cols:\n","        return '', 0 # invalid state (out of the grid) -> no action\n","\n","    else:\n","        # init best value and related action\n","        best_action = ' '\n","        best_value = -float('inf')\n","\n","        # for each possible action, we will simulate a move\n","        # and we will take the one that will maximize the reward\n","        for move, (x_move, y_move) in moves.items():\n","            new_pos = (x + x_move, y + y_move)\n","\n","            # check for legal move\n","            if 0 <= new_pos[0] < num_rows and 0 <= new_pos[1] < num_cols:\n","                value = grid[pos] + value_func[new_pos]\n","                if value > best_value:\n","                    best_action = move\n","                    best_value = value\n","        \n","        return best_action, best_value"]},{"cell_type":"code","execution_count":8,"metadata":{"cell_id":"dc35dfcbb0ef40ca895492fedb2f4802","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":89,"execution_start":1696083315246,"source_hash":"4159a26c"},"outputs":[],"source":["def find_opt_policy(max_iterations=5, policy_update_log=False):\n","    # init optimal policy\n","    opt_policy = {\n","        (0, 3): '', (1, 3): '', (2, 3): '',\n","        (0, 2): '', (1, 2): '', (2, 2): '',\n","        (0, 1): '', (1, 1): '', (2, 1): '',\n","        (0, 0): '', (1, 0): '', (2, 0): '',\n","    }\n","\n","    # init value_func (the grid where it will be shown the comulative rewards)\n","    value_func = {\n","        (0, 3): 0, (1, 3): 0, (2, 3): 0,\n","        (0, 2): 0, (1, 2): 0, (2, 2): 0,\n","        (0, 1): 0, (1, 1): 0, (2, 1): 0,\n","        (0, 0): 0, (1, 0): 0, (2, 0): 0,\n","    }\n","    prev_value_func = value_func\n","\n","    for i in range(max_iterations):\n","        for x in range(num_rows):\n","            for y in range(num_cols):\n","                best_action, new_value = calculate_state_value((x, y), value_func)\n","                value_func[(x, y)] = new_value\n","                opt_policy[(x, y)] = best_action\n","        \n","        if policy_update_log:\n","            print(f'Policy at iteration {i}:')\n","            print_policy(opt_policy)\n","            print('--------')\n","\n","        \n","    return opt_policy"]},{"cell_type":"code","execution_count":9,"metadata":{"cell_id":"54a1dba9aef74385ad8562cd18dc1480","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":71,"execution_start":1696083315264,"source_hash":"1c167553"},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimal policy: \n","\n","E | S |   | \n","N | N | N | \n","N | N | N | \n","E | E | N | \n"]}],"source":["opt_policy = find_opt_policy()\n","\n","print('Optimal policy: \\n')\n","\n","# print the optimal policy grid\n","print_policy(opt_policy)"]},{"cell_type":"markdown","metadata":{"cell_id":"af027e5bc9804d1a85239c6e80fac67e","deepnote_cell_type":"markdown"},"source":["---"]},{"cell_type":"markdown","metadata":{"cell_id":"7322bdd424c1411fa98c110b1b90dd3d","deepnote_cell_type":"markdown"},"source":["### Question 1c"]},{"cell_type":"markdown","metadata":{"cell_id":"a8ded23f72bb42e0b4b052d7285b936a","deepnote_cell_type":"markdown"},"source":["Following the optimal policy found in the previous queestions, the expected total reward will be:"]},{"cell_type":"code","execution_count":10,"metadata":{"cell_id":"34128f72397447e6be82c6ffa8d4a7a4","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":61,"execution_start":1696083315280,"source_hash":"95a33e5d"},"outputs":[],"source":["def total_expected_reward(policy):\n","    pos = start\n","    tot_reward = 0\n","\n","    while pos != end:\n","        x, y = pos\n","        if x < 0 or x >= num_rows or y < 0 or y >= num_cols:\n","            print('optimal policy break the constraints!')\n","            print(f'It stopped at point {pos}')\n","            return\n","\n","        tot_reward += grid[pos]\n","        pos = (pos[0] + moves[policy[pos]][0], pos[1] + moves[policy[pos]][1])\n","\n","    return tot_reward"]},{"cell_type":"code","execution_count":11,"metadata":{"cell_id":"c39a4aad632045939770f7dd2d382d75","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":81,"execution_start":1696083315294,"source_hash":"e855a6ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["Expected total reward for the optimal policy found: 0\n"]}],"source":["print(f'Expected total reward for the optimal policy found: {total_expected_reward(opt_policy)}')"]},{"cell_type":"markdown","metadata":{"cell_id":"3ec93b11e3464b1ab4d18cee6ae0478c","deepnote_cell_type":"markdown"},"source":["---"]},{"cell_type":"markdown","metadata":{"cell_id":"d5ef85773479496985435f28ca2c0839","deepnote_cell_type":"markdown","id":"sNkIk-k7qItT"},"source":["## Value Iteration"]},{"cell_type":"markdown","metadata":{"cell_id":"321a91e0b7a74ca2b906c0acf263070c","deepnote_cell_type":"markdown","id":"NJTFDikEqItT"},"source":["For larger problems we need to utilize algorithms to determine the optimal policy $\\pi^*$. *Value iteration* is one such algorithm that iteratively computes the value for each state. Recall that for a policy to be optimal, it must satisfy the Bellman equation above, meaning that plugging in a given candidate $V^*$ in the right-hand side (RHS) of the Bellman equation should result in the same $V^*$ on the left-hand side (LHS). This property will form the basis of our algorithm. Essentially, it can be shown that repeated application of the RHS to any intial value function $V^0(s)$ will eventually lead to the value $V$ which statifies the Bellman equation. Hence repeated application of the Bellman equation will also lead to the optimal value function. We can then extract the optimal policy by simply noting what actions that satisfy the equation.    "]},{"cell_type":"markdown","metadata":{"cell_id":"e8325beea517448780317dcba4ab2d10","deepnote_cell_type":"markdown","id":"3ZdhW0AZDoZv"},"source":["The process of repeated application of the Bellman equation is what we here call the _value iteration_ algorithm. It practically procedes as follows:\n","\n","```\n","epsilon is a small value, threshold\n","for x from i to infinity \n","do\n","    for each state s\n","    do\n","        V_k[s] = max_a Σ_s' p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\n","    end\n","    if  |V_k[s]-V_k-1[s]| < epsilon for all s\n","        for each state s,\n","        do\n","            π(s)=argmax_a ∑_s′ p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\n","            return π, V_k \n","        end\n","end\n","\n","```"]},{"cell_type":"markdown","metadata":{"cell_id":"3cfb9bea5c2c487ca9947c5d94dcf095","deepnote_cell_type":"markdown","id":"Nz3UqgozqItU"},"source":["**Example:** We will illustrate the value iteration algorithm by going through two iterations. Below is a 3x3 grid with the rewards given in each state. Assume now that given a certain state $s$ and action $a$, there is a probability 0.8 that that action will be performed and a probability 0.2 that no action is taken. For instance, if we take action **E** in state $(x,y)$ we will go to $(x+1,y)$ 80 percent of the time (given that that action is available in that state), and remain still 20 percent of the time. We will use have a discount factor $\\gamma = 0.9$. Let the initial value be $V^0(s)=0$ for all states $s\\in S$. \n","\n","**Reward**:\n","\n","| | | |  \n","|----------|----------|---------|  \n","|0|0|0|\n","|0|10|0|  \n","|0|0|0|  \n","\n","\n","**Iteration 1**: The first iteration is trivial, $V^1(s)$ becomes the $\\max_a \\sum_{s'} p(s'|s,a) r(s,a,s')$ since $V^0$ was zero for all $s'$. The updated values for each state become\n","\n","| | | |  \n","|----------|----------|---------|  \n","|0|8|0|\n","|8|2|8|  \n","|0|8|0|  \n","  \n","**Iteration 2**:  \n","  \n","Staring with cell (0,0) (lower left corner): We find the expected value of each move:  \n","Action **S**: 0  \n","Action **E**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \n","Action **N**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \n","Action **W**: 0\n","\n","Hence any action between **E** and **N** would be best at this stage.\n","\n","Similarly for cell (1,0):\n","\n","Action **N**: 0.8( 10 + 0.9 \\* 2) + 0.2(0 + 0.9 \\* 8) = 10.88 (Action **N** is the maximizing action)  \n","\n","Similar calculations for remaining cells give us:\n","\n","| | | |  \n","|----------|----------|---------|  \n","|5.76|10.88|5.76|\n","|10.88|8.12|10.88|  \n","|5.76|10.88|5.76|  \n"]},{"cell_type":"markdown","metadata":{"cell_id":"0d77c6e8407f40c9966b0d02451c2067","deepnote_cell_type":"markdown","id":"S3vIdFpuqItU"},"source":["## Question 2\n","\n","**2a)** Code the value iteration algorithm just described here, and show the converging optimal value function and the optimal policy for the above 3x3 grid.\n","\n","**2b)** Explain why the result of 2a) does not depend on the initial value $V_0$.\n","\n","**2c)** Describe your interpretation of the discount factor $\\gamma$. What would happen in the two extreme cases $\\gamma = 0$ and $\\gamma = 1$? Given some MDP, what would be important things to consider when deciding on which value of $\\gamma$ to use?"]},{"cell_type":"markdown","metadata":{"cell_id":"ab8259291d25409c9a85a1fafc0475a2","deepnote_cell_type":"markdown"},"source":["### Question 2a"]},{"cell_type":"code","execution_count":12,"metadata":{"cell_id":"40cf0a22c0f94f66bfb627d35c2ec947","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":29,"execution_start":1696083315385,"source_hash":"1a0fa6b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimal policy: \n"," [['S' 'S' 'S']\n"," ['E' 'N' 'W']\n"," ['N' 'N' 'N']] \n","\n","Related value function: \n"," [[45.568414   51.90726253 45.57554354]\n"," [51.90726253 48.01456793 51.91379617]\n"," [45.57554354 51.91379617 45.58153108]] \n","\n","Took 41 iterations to converge.\n"]}],"source":["def get_possible_actions(actions, pos, max_dims):\n","    x, y = pos\n","    possible = {}\n","        \n","    for action_name, (x_action, y_action) in actions.items():\n","        new_pos = (x + x_action, y + y_action)\n","        if 0 <= new_pos[0] < max_dims[0] and 0 <= new_pos[1] < max_dims[1]:\n","            possible[action_name] = (x_action, y_action)\n","            \n","    return possible\n","\n","def best_action_value(pos_current, V, rewards, actions, gamma, action_prob):\n","    # init best value and related action\n","    best_action = ''\n","    best_value = -float('inf')\n","    \n","    actions = get_possible_actions(actions, pos_current, rewards.shape)\n","    \n","    # for each possible action, we will simulate a move\n","    # and we will take the one that will maximize the reward\n","    for action_name, (i_action, j_action) in actions.items():\n","        reward_current = rewards[pos_current[0], pos_current[1]]\n","        value_current = V[pos_current[0], pos_current[1]]\n","        \n","        pos_next = (pos_current[0] + i_action, pos_current[1] + j_action)\n","        reward_next = rewards[pos_next[0], pos_next[1]]\n","        value_next = V[pos_next[0], pos_next[1]]\n","        \n","        value = (action_prob * (reward_next + gamma * value_next)) + \\\n","            ((1 - action_prob) * (reward_current + gamma * value_current))\n","        \n","        if value > best_value:\n","            best_action = action_name\n","            best_value = value\n","    \n","    return best_action, best_value\n","\n","def value_iteration_in_grid(rewards, action_prob, epsilon, gamma):\n","\n","    actions = {'N': (-1, 0), 'S': (1, 0), 'E': (0, 1), 'W': (0, -1)}\n","    n_rows, n_cols = rewards.shape\n","\n","    # init value matrix in for the previous and current iterations\n","    optimal_policy = np.empty((n_rows, n_cols), dtype='str')\n","    optimal_policy.fill('')\n","\n","    # init V for the current iteration\n","    V_current = np.zeros((n_rows, n_cols))\n","    V_prev = np.zeros((n_rows, n_cols))\n","\n","    iteration = 0\n","    delta = float('inf')\n","\n","    while delta > epsilon and iteration < 1000:\n","        # loop through all the states to compute\n","        # the value function in all states\n","        for i in range(n_rows):\n","            for j in range(n_cols):\n","                optimal_policy[i, j], V_current[i, j] = best_action_value(\n","                    pos_current=(i, j), V=V_current,\n","                    rewards=rewards, actions=actions,\n","                    gamma=gamma, action_prob=action_prob\n","                )\n","\n","        delta = np.max(np.abs(V_current - V_prev))\n","        V_prev = V_current.copy()\n","        \n","        iteration += 1\n","    \n","    return optimal_policy, V_current, iteration\n","\n","\n","rewards = np.array([\n","    [0, 0, 0],\n","    [0, 10, 0],\n","    [0, 0, 0]\n","])\n","\n","optimal_policy, V_func, iters = value_iteration_in_grid(\n","    rewards=rewards, \n","    action_prob=0.8, \n","    epsilon=1e-2,\n","    gamma=0.9,\n",")\n","\n","print('Optimal policy: \\n', optimal_policy, '\\n')\n","print('Related value function: \\n', V_func, '\\n')\n","print(f\"Took {iters} iterations to converge.\")"]},{"cell_type":"markdown","metadata":{"cell_id":"4b837498c34d458d8fe13abfd478f93f","deepnote_cell_type":"markdown"},"source":["### Question 2b"]},{"cell_type":"markdown","metadata":{"cell_id":"83dc72a595044d3dbed7d2dfd4f90783","deepnote_cell_type":"markdown"},"source":["The result of the convergence obtained in the previous question does not depend on the initial value $V_0$ mainly for the following reasons.\n","\n","- The main goal of a value iteration algortithm is to find the optimal policy and state values that maximize the objective function (that is, the expected cumulative reward). For this, the algortithm will focus on the long-terms outcomes towards the convergence, rather than base the goal on where it starts.\n","\n","- In addition to the first point, even though $V_0$ can influence initial agent exploration of the possible optimal policy, the algortithm used is an iterative one: this means that the value state will be refined more and more based on the state value found and the related iterative updates, rather from the start.\n","\n","- finally, since the value iteration algorithm is designed to converge to an optimal policy (especially thanks to the **gamma** discount factor value and **epsilon** threshold value), the result will always be the same regardless of the initial value state."]},{"cell_type":"markdown","metadata":{"cell_id":"32a0a84b2f6d4a15a569bb3aeacc2fad","deepnote_cell_type":"markdown"},"source":["### Question 2c"]},{"cell_type":"markdown","metadata":{"cell_id":"9691dc4bd82846168e9098f18ced010d","deepnote_cell_type":"markdown"},"source":["The **discount factor** $\\gamma$ ( $0 \\leq \\gamma < 1$ ) is a parameter used in the MDP Bellman equation to deincentivize the agent to get long-term rewards, making the earliest ones more important in order to achieve the goal of maximizing the reward fuction.\n","\n","This value is especially used in the context of an MDP where the interaction sequence is not finite (that is, with a time variable $t$ such that $t \\to +\\infty$), and thus the sum of the accumulated reward won't converge. Adding $\\gamma$ to the equation would let the sum converge even with an infinite interaction sequence, since future rewards will be less important than the immediate ones. What will be maximized, in this case, is the **utility** of the reward sequence $r_t, r_{t+1}, r_{t+2}, ...$ defined as: \n","$U_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2}, ...$\n","\n","There can be 2 different cases to consider, based on the value of the discount factor: $\\gamma = 1$ and $\\gamma = 0$.\n","\n","- if $\\gamma = 1$, the agent would consider again the future rewards with greater weight. As mentioned before, since It's a case with an MDP with an infinite interaction sequence, highly considering long-term rewards would let the value iteration algorithm converge very slowly or even diverge.   \n","- if $\\gamma = 0$, instead, long-term rewards won't give any benefits and the immediate ones will be the only ones taken into account. The consquence of this agent behavior is that, even though the value iteration algorithm could converge really quickly and find a possible suitable policy, that policy will most probably suboptimal. This for the lack of exploration of new possible policies, as well as collecting future rewards.\n","\n","What will be shown now is a plot with the value of the discount factor in relation to the related number of iterations required. This is to clearly visualize, depending on the value of $\\gamma$, how fast the value iteration algorithm can converge. Since setting $\\gamma = 1$ does not give the guarantee of a convergence, an iteration threshold could be set."]},{"cell_type":"code","execution_count":13,"metadata":{"cell_id":"08e94323c92047919301f41daba06b51","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":315,"execution_start":1696083315385,"source_hash":"10a93dcf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Gamma values: [0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n","Related number of iterations (with a max iterations = 1000): [   2.    4.    5.    6.    7.    8.   10.   14.   20.   41. 1000.]\n"]}],"source":["gammas = np.arange(0, 1.1, 0.1)\n","iterations_list = np.array([])\n","\n","for gamma in gammas:\n","    optimal_policy, V_func, iters = value_iteration_in_grid(\n","        rewards=rewards, \n","        action_prob=0.8, \n","        epsilon=1e-2,\n","        gamma=gamma,\n","    )\n","    iterations_list = np.append(iterations_list, iters)\n","\n","print(f'Gamma values: {gammas}')\n","print(f'Related number of iterations (with a max iterations = 1000): {iterations_list}')"]},{"cell_type":"code","execution_count":14,"metadata":{"cell_id":"6b06442c8c554599b4986a18d378ff1e","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":777,"execution_start":1696083315724,"source_hash":"d3b9049c"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{"image/png":{"height":455,"width":580}},"output_type":"display_data"}],"source":["sns.lineplot(x=gammas, y=iterations_list)\n","\n","plt.title(\"Discont factor and number of iterations\")\n","\n","plt.xlabel(\"Discount factor\")\n","plt.ylabel(\"Number of iterations to converge\")\n","\n","plt.axline(xy1=(0, 1000), slope=0, linestyle='--')\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"172b022bc58043338edcba2940981b37","deepnote_cell_type":"markdown","id":"v9tL23YlqItU"},"source":["## Reinforcement Learning (RL) (Theory for optional question 3)\n","Until now, we understood that knowing the MDP, specifically $p(s'|a,s)$ and $r(s,a,s')$ allows us to efficiently find the optimal policy using the value iteration algorithm. Reinforcement learning (RL) or decision making under uncertainity, however, arises from the question of making optimal decisions without knowing the true world model (the MDP in this case).\n","\n","So far we have defined the value function for a policy through $V^\\pi$. Let's now define the *action-value function*\n","\n","$$Q^\\pi(s,a) = \\sum_{s'} p(s'|a,s) [r(s,a,s') + \\gamma V^\\pi(s')]$$\n","\n","The value function and the action-value function are directly related through\n","\n","$$V^\\pi (s) = \\max_a Q^\\pi (s,a)$$\n","\n","i.e, the value of taking action $a$ in state $s$ and then following the policy $\\pi$ onwards. Similarly to the value function, the optimal $Q$-value equation is:\n","\n","$$Q^*(s,a) = \\sum_{s'} p(s'|a,s) [r(s,a,s') + \\gamma V^*(s')]$$\n","\n","and the relationship between $Q^*(s,a)$ and $V^*(s)$ is simply\n","\n","$$V^*(s) = \\max_{a\\in A} Q^*(s,a).$$\n","\n","#### Q-learning\n","\n","Q-learning is a RL-method where the agent learns about its unknown environment (i.e., the MDP is unknown) through exploration. In each time step *t* the agent chooses an action *a* based on the current state *s*, observes the reward *r* and the next state *s'*, and repeats the process in the new state. Q-learning is then a method that allows the agent to act optimally. Here we will focus on the simplest form of Q-learning algorithms, which can be applied when all states are known to the agent, and the state and action spaces are reasonably small. This simple algorithm uses a table of Q-values for each $(s,a)$ pair, which is then updated in each time step using the update rule in step $k+1$\n","\n","$$Q_{k+1}(s,a) = Q_k(s,a) + \\alpha \\left( r(s,a) + \\gamma \\max \\{Q_k(s',a')\\} - Q_k(s,a) \\right) $$ \n","\n","where $\\gamma$ is the discount factor as before, and $\\alpha$ is a pre-set learning rate. It can be shown that this algorithm converges to the optimal policy of the underlying MDP for certain values of $\\alpha$ as long as there  is sufficient exploration. For our case, we set a constant $\\alpha=0.1$.\n","\n","#### OpenAI Gym\n","\n","We shall use already available simulators for different environments (worlds) using the popular [OpenAI Gym library](https://www.gymlibrary.dev/). It just implements different types of simulators including ATARI games. Although here we will only focus on simple ones, such as the **Chain enviroment** illustrated below.\n","![alt text](Chain.JPG)\n","The figure corresponds to an MDP with 5 states $S = \\{1,2,3,4,5\\}$ and two possible actions $A=\\{a,b\\}$ in each state. The arrows indicate the resulting transitions for each state-action pair, and the numbers correspond to the rewards for each transition.\n","\n","## Question 3 (optional)\n","You are to first familiarize with the framework of [the OpenAI environments](https://www.gymlibrary.dev/), and then implement the Q-learning algorithm for the <code>NChain-v0</code> enviroment depicted above, using default parameters and a learning rate of $\\gamma=0.95$. Report the final $Q^*$ table after convergence of the algorithm. For an example on how to do this, you can refer to the Q-learning of the **Frozen lake environment** (<code>q_learning_frozen_lake.ipynb</code>), uploaded on Canvas. *Hint*: start with a small learning rate.\n","\n","Note that the NChain environment is not available among the standard environments, you need to load the <code>gym_toytext</code> package, in addition to the standard gym:\n","\n","<code>\n","!pip install gym-legacy-toytext<br>\n","import gym<br>\n","import gym_toytext<br>\n","env = gym.make(\"NChain-v0\")<br>\n","</code>"]},{"cell_type":"markdown","metadata":{"cell_id":"4fb806fab97941809fc54936119029b2","deepnote_cell_type":"markdown"},"source":["---"]},{"cell_type":"markdown","metadata":{"cell_id":"26df845b88de4ad8918a1469d68cf8f6","deepnote_cell_type":"markdown","id":"AfKSybVI-UN1"},"source":["## Question 4\n","\n","**4a)** What is the importance of exploration in reinforcement learning? Explain with an example.\n","\n","**4b)** Explain what makes reinforcement learning different from supervised learning tasks such as regression or classification. \n"]},{"cell_type":"markdown","metadata":{"cell_id":"760eeeeb4a154954801fd58a580503c3","deepnote_cell_type":"markdown"},"source":["---"]},{"cell_type":"markdown","metadata":{"cell_id":"47a4b04cccb94214919f176e5a0d2a05","deepnote_cell_type":"markdown"},"source":["### Question 4a"]},{"cell_type":"markdown","metadata":{"cell_id":"5ea760f483834841b5beba1fb5a8d442","deepnote_cell_type":"markdown"},"source":["*The exploration plays an important role in reinforcement learning, especially in the **discovery of optimal or near-optimal policies in an environment with limited or no prior knowledge.*** In particular, it would allow an agent to take a random action sometimes, instead of taking an action always based on its current knowledge. In this way the agent will collect more experience that can be used to find better optimal policies, avoid local optima and adapt to possible environment changes.\n","\n","*An example that highlights the importance of this concept could be, **training a self-driving car in a simulated urban environment.*** The vehicle is in an unknown environment (It does not know the the city layout, the position of the traffic lights or specific locations for example), and its goal is to maximize its reward in order to reach destinations as fast as possible while avoiding incident and following the traffic laws. Letting the autonomous car explore rather than only basing its decisions on its current knowledege will bring many benefits, for instance:\n","\n","- form a first prior knowledge about the environment layout, the traffic patterns, the location of places and also specific optimal routes. Without a first exploration the car wouldn't have knowledege and it would take actions randomly.\n","- even though discovering the optimal path could be the first priority to reach the car's goal, exploration would also allow the vehicle to discover different types of paths, from a safer one to a route with less traffic. In other words, the autonomous car knowledege would be more applicable to the real world and to users' requests. \n","\n","Despite the advanteges of exploring an environment, another important aspect is to balance it with the concept of exploitation, that is using the accumulated information to maximize the expected rewards. This because if an agent focuses too much in exploring a new possible path, it will never take advantage of the knowledege acquired through the episodes to follow an optimal policy in order to reach its goals."]},{"cell_type":"markdown","metadata":{"cell_id":"c7010be1b7f44992a12b0ba97bb4dd5c","deepnote_cell_type":"markdown"},"source":["### Question 4b"]},{"cell_type":"markdown","metadata":{"cell_id":"d882fbecf05f43fd91c059806b3ffe88","deepnote_cell_type":"markdown"},"source":["Compared to learning paradigms like supervised learning, what makes reinforcement learning different is especially the available resources, the goal and how it is reached.\n","\n","- In supervised learning, the *main objective is to predict something from given data*. We have an initial dataset with the datapoint features and the target variable, that is the value that we want to predict for a specific instance; the dataset will be used to train the algorithm in making predictions, from a real number or a category. In other words, we already have available (and labeled) information to tell the algorithm how to solve its task (regression and classification). \n","- In reinforcement learning, instead, we don't have neither labeled data nor a dataset at all. Starting with no knowledge the algorithm in this case will learn from an interactive environment, *with the goal to find a policy of actions to maximize a cumulative reward signal*. The way reinforcement learning works makes it more suitable in problems where an AI has to take sequential decisions, such as self-driving cars, game playing or even more robotics."]},{"cell_type":"markdown","metadata":{"cell_id":"8364afbee70247d89a869029601656d9","deepnote_cell_type":"markdown","id":"I1iFSvirqItV"},"source":["## Question 5\n","\n","**5a)** Give a summary of how a decision tree works and how it extends to random forests.\n","\n","**5b)** State at least one advantage and one drawback with using random forests over decision trees."]},{"cell_type":"markdown","metadata":{"cell_id":"e960bb1f83a641beb0f0f2f6535d7c28","deepnote_cell_type":"markdown"},"source":["---"]},{"cell_type":"markdown","metadata":{"cell_id":"2db584dd077347798be1707b80421cc1","deepnote_cell_type":"markdown"},"source":["### Question 5a"]},{"cell_type":"markdown","metadata":{"cell_id":"c026fdd650674517826260c9027e821d","deepnote_cell_type":"markdown"},"source":["A decision tree is a machine learning learning algorithm that can be used both for regression and classification tasks. It consists on a binary branching structure (or binary tree) to make a decision related to the task considered: each node contains a feature comparison among the ones in the starting dataset (if a feature is a specific category or it's greater than a value, for example), and the result of such comparison determines whether we should proceed in the right or left child of the given node. By bulding the tree in this way, we will recursively partition our data in subsets based on values of the input feature to make predictions. Building a decision tree typically follows the process below:\n","\n","1. we will start at a root node, with all of our examples\n","2. a feature from the dataset will be selected to spit the data into subset and create 2 child nodes of the trees. The criteria to determine which feature could work best to make a split is to first compute the so called *entropy*, a measure that takes into account a balance split and pure partition (how much the label of the node in that partition is disjointed); since our goal is to reduce the entropy of a split, we then calculate the *information gain*, which measures how much the entropy would be reduced selecting a specific feature.\n","3. we split the dataset into left and right branch, according to the choice in the 2nd step.\n","4. we keep repeating the process above until a termination criteria, which could be:\n","    - having 100% of one class in a node\n","    - information gain from additional splits is less than a threshold\n","    - number of examples would be below a threshold.\n","\n","Since there are a high number of possible decision trees that can be created starting from a dataset, an idea to improve the tree model even more is to extend this concept to bulding hundrends or also thousands of other different possible trees: we can then extract a possible label of a prediction by evaluating a test set to each of them. This algorithm is called *tree ensables* or *random forest* and overcome one the biggest drawback of using a single decision tree, which is overfitting.\n","To make sure that each tree in the random forest will be different, 2 techinques will be used:\n","\n","- instead of taking the entire dataset to start building the model, only a random sampling of data will be taken into consideration.\n","- for similar reasons in the first point, to not let the trees be too much highly correlated only a random subset of features will be used to build a tree (*bagging*).  "]},{"cell_type":"markdown","metadata":{"cell_id":"d0ee4ceb18264701884b9e53407b49ea","deepnote_cell_type":"markdown"},"source":["### Question 5b"]},{"cell_type":"markdown","metadata":{"cell_id":"e0991b5061574dacaa0aa52d39207258","deepnote_cell_type":"markdown"},"source":["As mentioned in the last question, using random forests over decision trees would lead to higher accuracy and a more robust method for the number of mutiple trees participating in the process. Furthermore, random forests does not suffer from the overfitting problem, since taking the average of all the predictions would cancels biases.\n","\n","Despite the advantages, there are also drawbacks. The main drawback of using random forests is the slow performance in making a prediction, sicne every tree has to be taken into account. The processes can be time-consuming. Moreover, the model can be harder to interpret compared to a single decision tree, where you can easly make a decision following the path in the trees."]},{"cell_type":"markdown","metadata":{"cell_id":"c4f142467dc5426c92414f79611bf044","deepnote_cell_type":"markdown","id":"-yHCotQGqItV"},"source":["\n","# References\n","Primer/text based on the following references:\n","* http://www.cse.chalmers.se/~chrdimi/downloads/book.pdf\n","* https://github.com/olethrosdc/ml-society-science/blob/master/notes.pdf"]}],"metadata":{"colab":{"provenance":[]},"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"84c1a79b07a947b09ea0cd6a2b98c5f8","deepnote_persisted_session":{"createdAt":"2023-09-30T09:46:50.239Z"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5 | packaged by conda-forge | (main, Aug 27 2023, 03:34:09) [GCC 12.3.0]"},"vscode":{"interpreter":{"hash":"55a254bb32391f07d2b66b3e7d35e22f8a3e92ba4d975664238fc05ad2b3d3a3"}}},"nbformat":4,"nbformat_minor":0}
